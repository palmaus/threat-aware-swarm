# Threat-aware Swarm — MARL-симулятор роя БПЛА с угрозами

Этот репозиторий — пет-проект по multi-agent reinforcement learning (MARL).
Идея проекта в следующем: рой дронов учится долетать до цели, выживать и вести себя как рой (не слипаться в одну вереницу), когда на поле есть зоны угроз.

Стек: PettingZoo (ParallelEnv) + SuperSuit + Stable-Baselines3 (PPO), с parameter sharing (одна политика на всех агентов).

---

## Что умеет проект

Это симулятор роя дронов в 2D-среде: есть ограниченное поле, заданные границы и статические (позже будут и динамические) угрозы. У каждой угрозы свой радиус действия и вероятность «смерти» агента, если он находится внутри зоны поражения на очередном тике.

Внутри среды работает рой из `N` агентов, которые обучаются **общей политике** на PPO — то есть все дроны действуют по одному и тому же «мозгу», просто в разных состояниях.

Система наград сделана так, чтобы поощрять движение к цели и удержание в целевой зоне (с дополнительным стимулом дожиматься ближе к центру), но при этом учитывать риск от угроз. За гибель агента предусмотрен отдельный штраф, а ещё есть наказание за слишком тесную близость к соседям — правда, в зоне цели этот штраф отключается, чтобы финальный «сбор» роя не превращался в проблему.

Для наблюдения за прогрессом всё логируется в TensorBoard: стандартные метрики PPO (train/rollout) дополняются swarm-метриками вроде количества живых, дошедших до цели, находящихся в goal-зоне, дистанций и прочего.

Проект также включает набор готовых скриптов: для обучения модели (`scripts/trained_ppo.py`), для ведения реестра моделей и массовой оценки по нему (`scripts/index_models.py`, `scripts/eval_models.py`), а также для визуализации и демонстрации поведения агентов (`viz/run_trained_pz.py`, `viz/run_demo.py`).

---

## Установка

Рекомендуемый Python: **3.10+**.

```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

Если запускаете скрипты из корня репозитория, удобно добавлять `PYTHONPATH=.`:

```bash
PYTHONPATH=. python -c "import env; print('ok')"
```

---

## Быстрый старт

### Обучение PPO (основной способ)

Главный скрипт обучения — `scripts/trained_ppo.py`.

Пример (команда из корня репо):

```bash
PYTHONPATH=. python scripts/trained_ppo.py \
  --run_dir runs/run_$(date +%Y%m%d_%H%M%S) \
  --timesteps 6000000 \
  --save_every 2000000 \
  --eval_every 20000 \
  --n_eval_episodes 20 \
  --max_steps 600
```

Что получится:
- `runs/<run_id>/models/` — чекпоинты и `best_by_finished.zip`
- `runs/<run_id>/tb/` — TensorBoard логи
- `runs/<run_id>/meta/run.json` — метаданные запуска

TensorBoard:

```bash
tensorboard --logdir runs
```

### 3) Визуализация обученной модели

Обычно запускаю так:

```bash
PYTHONPATH=. python -m viz.run_trained_pz
```

Внутри `viz/run_trained_pz.py` выбирается модель (по умолчанию — `best_by_finished` из последнего прогона).
Внутри скрипта есть аргумент `--model`, можно указать путь к конкретной модели прямо в командной строке:

```bash
PYTHONPATH=. python -m viz.run_trained_pz --model runs/run_20260118_203327/models/best_by_finished.zip
```

---

## Как сравниваются модели

1) Сформировать реестр:

```bash
PYTHONPATH=. python scripts/index_models.py --scan runs --out model_registry.csv --rewrite
```

2) Прогнать оценку (например, 20 эпизодов, фиксированный и случайный seed):

```bash
PYTHONPATH=. python scripts/eval_models.py --registry model_registry.csv --mode both --n-episodes 20 --max-steps 600 --deterministic --seed 0
```

В `model_registry.csv` добавятся колонки с результатами и ошибками (если какие-то модели несовместимы).

---

## Заметки по воспроизводимости

В проекте специально хранятся метаданные каждого прогона (`runs/<id>/meta/run.json`), потому что по одному TensorBoard потом тяжело восстановить:
- какими были гиперпараметры;
- какая версия среды была;
- какие флаги были включены.

Notebook’ (`notebooks/`) используется как лаборатория: там графики, запуски прогонов.
Скриптом для запуска обучения является `scripts/trained_ppo.py`.
